Traceback (most recent call last):
  File "/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/jupyter_cache/executors/utils.py", line 51, in single_nb_execution
    executenb(
  File "/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/nbclient/client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "/opt/miniconda3/envs/tf2/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/nbclient/client.py", line 663, in async_execute
    await self.async_execute_cell(
  File "/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/nbclient/client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/nbclient/client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import pandas as pd
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
from matplotlib.ticker import StrMethodFormatter
from webcolors import CSS3_NAMES_TO_HEX
import seaborn as sns
sns.set(style='white', context='paper')

from os import listdir
from os.path import isfile, join
from PIL import Image
from PIL.ImageStat import Stat
import math

import requests
from bs4 import BeautifulSoup
from os.path  import basename

########### 1. Collect data from the Art Gallery of NSW website ###########
# global mainURL
# mainURL = 'https://www.artgallery.nsw.gov.au/'

# def assort_prize_metadata(text):
#     prize_dict = dict({'Entries':'',
#                        'Presenting partner':'',
#                        'Sponsor':'',
#                        'Exhibition dates':'', 
#                        'Misc.':'',
#                        'Text':''})
#     for t in text:
#         for k in list(prize_dict.keys())[:-2]:
#             if k in t: 
#                 if '  ' not in t: prize_dict[k] = t.strip().replace(k + ': ','')
#                 else: 
#                     prize_dict[k] = t.split('  ')[0]
#                     prize_dict['Text'] = t.split(prize_dict[k])[1]
#                 break

#     if prize_dict['Text'] == '':
#         prize_dict['Misc.'] = text[-1].split('  ')[0]
#         if len(prize_dict['Misc.']): 
#             prize_dict['Text'] = text[-1].split(prize_dict['Misc.'])[1]    
        
#     return prize_dict

# def collect_records(prize = 'archibald', prize_year = 1921):
#     prize_url = mainURL + "prizes/" + prize + '/' + str(prize_year)
#     page = requests.get(prize_url)
#     soup = BeautifulSoup(page.content, "html.parser")
    
#     # fetch winner data
#     try:
#         winner_artist = soup.find_all("span", class_="card-prizesWinner-artist")[0].text
#         winner_title = soup.find_all("span", class_="card-prizesWinner-title")[0].text
        
#         try: 
#             winner_image = soup.find_all("img", class_="card-prizesWinner-image")[0].get('src')
#             with open('ArchibaldWinners/' + str(yr) + '_' + basename(mainURL + winner_image), "wb") as f: 
#                 f.write(requests.get(mainURL + winner_image).content)
#         except: winner_image = None
        
#         winner_info = [winner_artist,winner_title,winner_image]
#     except:
#         winner_info = [None,None,None]
    
#     # download winning image
#     # with open(basename(winner_image),"wb") as f: f.write(requests.get(mainURL + winner_image).content)

#     # pre-process
#     delimiter = '###'                           # unambiguous string
#     for line_break in soup.findAll('br'):       # loop through line break tags
#         line_break.replaceWith(delimiter)       # replace br tags with delimiter
#     textModule = soup.find("div", class_="grid text").get_text().split(delimiter)  # get list of strings
    
#     # fetch prize metadata
#     prize_metadata_dict = assort_prize_metadata(text=textModule)
#     prize_metadata_dict['winner_info'] = winner_info
    
#     # fetch participant data
#     participants = []
    
#     if len(soup.find_all("div", class_="grid text")) > 1:
#         for item in soup.find_all("div", class_="grid text")[1].find_all('ul')[0].find_all('li'):
            
#             try: participant_href = item.find_all("a")[0].get('href')
#             except: participant_href = ''
                
#             participant_artist = item.find_all("strong")[0].text
#             participant_title = item.find_all("em")[0].text
            
#             try: participant_label = item.text.split(participant_title)[-1].strip()
#             except: participant_label = ''
                
#             participants.append([participant_href, participant_artist, participant_title, participant_label])
#     else:
#         for item in soup.find_all("div", class_="artworksList-item"):
#             participant_href = item.find_all("a", class_="card-artwork-link")[0].get('href')
#             participant_artist = item.find_all("span", class_="card-artwork-artist")[0].text
#             participant_title = item.find_all("span", class_="card-artwork-title")[0].text
#             participant_label = item.find_all("p", class_="card-artwork-label")[0].text
#             participants.append([participant_href, participant_artist, participant_title, participant_label])
            
#     prize_metadata_dict['participant_info'] = participants
#     return prize_metadata_dict

# archibald_data_dict = dict({'Prize Data':[],'Year':[]})

# # pre 1991/92
# for yr in range(1921,1991):
#     try: archibald_data_dict['Prize Data'].append(
#         collect_records(prize = 'archibald', prize_year = yr))
#     except: archibald_data_dict['Prize Data'].append(None)
#     archibald_data_dict['Year'].append(yr)

# # 1991/92 exception
# try: archibald_data_dict['Prize Data'].append(
#     collect_records(prize = 'archibald', prize_year = '1991-92'))
# except: archibald_data_dict['Prize Data'].append(None)
# archibald_data_dict['Year'].append('1992')

# # post 1991/92
# for yr in range(1993,2023):
#     try: archibald_data_dict['Prize Data'].append(
#         collect_records(prize = 'archibald', prize_year = yr))
#     except: archibald_data_dict['Prize Data'].append(None)
#     archibald_data_dict['Year'].append(yr)

########### Convert dictionary as dataframe and write as csv file ###########
# archies = pd.DataFrame(archibald_data_dict)
# archies.to_csv('data/archies.csv', index=False)

########### Read csv file as dataframe ###########
# this imported dataset was further preprocessed by filtering on winners 
# and adding columns in regard to each winner's biographical information
# along with corresponding ANZSCO classification data
archies = pd.read_csv('data/archies_v2.csv')

# We show a transposed of the first three rows of the dataframe
archies.head(3).T
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
Cell [0;32mIn [1], line 138[0m
[1;32m     19[0m [38;5;28;01mfrom[39;00m [38;5;21;01mos[39;00m[38;5;21;01m.[39;00m[38;5;21;01mpath[39;00m  [38;5;28;01mimport[39;00m basename
[1;32m     21[0m [38;5;66;03m########### 1. Collect data from the Art Gallery of NSW website ###########[39;00m
[1;32m     22[0m [38;5;66;03m# global mainURL[39;00m
[1;32m     23[0m [38;5;66;03m# mainURL = 'https://www.artgallery.nsw.gov.au/'[39;00m
[0;32m   (...)[0m
[1;32m    136[0m [38;5;66;03m# and adding columns in regard to each winner's biographical information[39;00m
[1;32m    137[0m [38;5;66;03m# along with corresponding ANZSCO classification data[39;00m
[0;32m--> 138[0m archies [38;5;241m=[39m [43mpd[49m[38;5;241;43m.[39;49m[43mread_csv[49m[43m([49m[38;5;124;43m'[39;49m[38;5;124;43mdata/archies_v2.csv[39;49m[38;5;124;43m'[39;49m[43m)[49m
[1;32m    140[0m [38;5;66;03m# We show a transposed of the first three rows of the dataframe[39;00m
[1;32m    141[0m archies[38;5;241m.[39mhead([38;5;241m3[39m)[38;5;241m.[39mT

File [0;32m/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/util/_decorators.py:211[0m, in [0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper[0;34m(*args, **kwargs)[0m
[1;32m    209[0m     [38;5;28;01melse[39;00m:
[1;32m    210[0m         kwargs[new_arg_name] [38;5;241m=[39m new_arg_value
[0;32m--> 211[0m [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/util/_decorators.py:317[0m, in [0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper[0;34m(*args, **kwargs)[0m
[1;32m    311[0m [38;5;28;01mif[39;00m [38;5;28mlen[39m(args) [38;5;241m>[39m num_allow_args:
[1;32m    312[0m     warnings[38;5;241m.[39mwarn(
[1;32m    313[0m         msg[38;5;241m.[39mformat(arguments[38;5;241m=[39marguments),
[1;32m    314[0m         [38;5;167;01mFutureWarning[39;00m,
[1;32m    315[0m         stacklevel[38;5;241m=[39mfind_stack_level(inspect[38;5;241m.[39mcurrentframe()),
[1;32m    316[0m     )
[0;32m--> 317[0m [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950[0m, in [0;36mread_csv[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)[0m
[1;32m    935[0m kwds_defaults [38;5;241m=[39m _refine_defaults_read(
[1;32m    936[0m     dialect,
[1;32m    937[0m     delimiter,
[0;32m   (...)[0m
[1;32m    946[0m     defaults[38;5;241m=[39m{[38;5;124m"[39m[38;5;124mdelimiter[39m[38;5;124m"[39m: [38;5;124m"[39m[38;5;124m,[39m[38;5;124m"[39m},
[1;32m    947[0m )
[1;32m    948[0m kwds[38;5;241m.[39mupdate(kwds_defaults)
[0;32m--> 950[0m [38;5;28;01mreturn[39;00m [43m_read[49m[43m([49m[43mfilepath_or_buffer[49m[43m,[49m[43m [49m[43mkwds[49m[43m)[49m

File [0;32m/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605[0m, in [0;36m_read[0;34m(filepath_or_buffer, kwds)[0m
[1;32m    602[0m _validate_names(kwds[38;5;241m.[39mget([38;5;124m"[39m[38;5;124mnames[39m[38;5;124m"[39m, [38;5;28;01mNone[39;00m))
[1;32m    604[0m [38;5;66;03m# Create the parser.[39;00m
[0;32m--> 605[0m parser [38;5;241m=[39m [43mTextFileReader[49m[43m([49m[43mfilepath_or_buffer[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwds[49m[43m)[49m
[1;32m    607[0m [38;5;28;01mif[39;00m chunksize [38;5;129;01mor[39;00m iterator:
[1;32m    608[0m     [38;5;28;01mreturn[39;00m parser

File [0;32m/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442[0m, in [0;36mTextFileReader.__init__[0;34m(self, f, engine, **kwds)[0m
[1;32m   1439[0m     [38;5;28mself[39m[38;5;241m.[39moptions[[38;5;124m"[39m[38;5;124mhas_index_names[39m[38;5;124m"[39m] [38;5;241m=[39m kwds[[38;5;124m"[39m[38;5;124mhas_index_names[39m[38;5;124m"[39m]
[1;32m   1441[0m [38;5;28mself[39m[38;5;241m.[39mhandles: IOHandles [38;5;241m|[39m [38;5;28;01mNone[39;00m [38;5;241m=[39m [38;5;28;01mNone[39;00m
[0;32m-> 1442[0m [38;5;28mself[39m[38;5;241m.[39m_engine [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_make_engine[49m[43m([49m[43mf[49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mengine[49m[43m)[49m

File [0;32m/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1729[0m, in [0;36mTextFileReader._make_engine[0;34m(self, f, engine)[0m
[1;32m   1727[0m     is_text [38;5;241m=[39m [38;5;28;01mFalse[39;00m
[1;32m   1728[0m     mode [38;5;241m=[39m [38;5;124m"[39m[38;5;124mrb[39m[38;5;124m"[39m
[0;32m-> 1729[0m [38;5;28mself[39m[38;5;241m.[39mhandles [38;5;241m=[39m [43mget_handle[49m[43m([49m
[1;32m   1730[0m [43m    [49m[43mf[49m[43m,[49m
[1;32m   1731[0m [43m    [49m[43mmode[49m[43m,[49m
[1;32m   1732[0m [43m    [49m[43mencoding[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43moptions[49m[38;5;241;43m.[39;49m[43mget[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mencoding[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m)[49m[43m,[49m
[1;32m   1733[0m [43m    [49m[43mcompression[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43moptions[49m[38;5;241;43m.[39;49m[43mget[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mcompression[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m)[49m[43m,[49m
[1;32m   1734[0m [43m    [49m[43mmemory_map[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43moptions[49m[38;5;241;43m.[39;49m[43mget[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mmemory_map[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[38;5;28;43;01mFalse[39;49;00m[43m)[49m[43m,[49m
[1;32m   1735[0m [43m    [49m[43mis_text[49m[38;5;241;43m=[39;49m[43mis_text[49m[43m,[49m
[1;32m   1736[0m [43m    [49m[43merrors[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43moptions[49m[38;5;241;43m.[39;49m[43mget[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mencoding_errors[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[38;5;124;43m"[39;49m[38;5;124;43mstrict[39;49m[38;5;124;43m"[39;49m[43m)[49m[43m,[49m
[1;32m   1737[0m [43m    [49m[43mstorage_options[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43moptions[49m[38;5;241;43m.[39;49m[43mget[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mstorage_options[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m)[49m[43m,[49m
[1;32m   1738[0m [43m[49m[43m)[49m
[1;32m   1739[0m [38;5;28;01massert[39;00m [38;5;28mself[39m[38;5;241m.[39mhandles [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m
[1;32m   1740[0m f [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mhandles[38;5;241m.[39mhandle

File [0;32m/opt/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/io/common.py:857[0m, in [0;36mget_handle[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)[0m
[1;32m    852[0m [38;5;28;01melif[39;00m [38;5;28misinstance[39m(handle, [38;5;28mstr[39m):
[1;32m    853[0m     [38;5;66;03m# Check whether the filename is to be opened in binary mode.[39;00m
[1;32m    854[0m     [38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.[39;00m
[1;32m    855[0m     [38;5;28;01mif[39;00m ioargs[38;5;241m.[39mencoding [38;5;129;01mand[39;00m [38;5;124m"[39m[38;5;124mb[39m[38;5;124m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m ioargs[38;5;241m.[39mmode:
[1;32m    856[0m         [38;5;66;03m# Encoding[39;00m
[0;32m--> 857[0m         handle [38;5;241m=[39m [38;5;28;43mopen[39;49m[43m([49m
[1;32m    858[0m [43m            [49m[43mhandle[49m[43m,[49m
[1;32m    859[0m [43m            [49m[43mioargs[49m[38;5;241;43m.[39;49m[43mmode[49m[43m,[49m
[1;32m    860[0m [43m            [49m[43mencoding[49m[38;5;241;43m=[39;49m[43mioargs[49m[38;5;241;43m.[39;49m[43mencoding[49m[43m,[49m
[1;32m    861[0m [43m            [49m[43merrors[49m[38;5;241;43m=[39;49m[43merrors[49m[43m,[49m
[1;32m    862[0m [43m            [49m[43mnewline[49m[38;5;241;43m=[39;49m[38;5;124;43m"[39;49m[38;5;124;43m"[39;49m[43m,[49m
[1;32m    863[0m [43m        [49m[43m)[49m
[1;32m    864[0m     [38;5;28;01melse[39;00m:
[1;32m    865[0m         [38;5;66;03m# Binary mode[39;00m
[1;32m    866[0m         handle [38;5;241m=[39m [38;5;28mopen[39m(handle, ioargs[38;5;241m.[39mmode)

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'data/archies_v2.csv'
FileNotFoundError: [Errno 2] No such file or directory: 'data/archies_v2.csv'

